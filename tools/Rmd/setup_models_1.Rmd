---
title: "BGS under non-equilibrium demography (paper 1)"
author: "Gustavo V. Barroso"
date: "`r Sys.Date()`"
output:
  pdf_document:
  toc: true
number_sections: true
toc_depth: 1
---
  
```{r setup, include=F, message=F, warning=F}

knitr::opts_chunk$set(echo=TRUE)

library(tidyverse)
library(bigsnpr) # for seq_log
library(data.table)

library(reticulate)
use_python("/usr/bin/python3")

knitr::opts_knit$set(root.dir="~/Data/momentspp/paper_1/study")
```

This is an empty python chunk.
Apparently this must exist in order for reticulate to be able to read
R data frames in future python chunks.

```{python}
```

# Creating folders and files

This builds a look-up table on a dense parameter grid that will allow for fast 
assembly of whole chromosomes

```{r, params, include=F, eval=T, results=F, message=F, warning=F}

Na <- 1e+5 # ancestral N
N1 <- c(Na / 10) # current N
N2 <- c(N1, Na)
t <- 250000 # in generations

options(digits=16)

# rounding is required so that directory names don't diverge due to precision
lookup_u <- round(seq_log(from=4e-9, to=1e-8, length.out=5), digits=10)
lookup_r <- round(sort(c(seq_log(from=1e-8, to=1e-3, length.out=10),
                  seq(from=1e-4, to=1e-2, length.out=10))), digits=10)
lookup_s <- round(-sort(c(seq_log(from=1e-5, to=1e-4, length.out=10), 
                  seq_log(from=7.5e-5, to=1e-3, length.out=10))), digits=10)

# for discretizing Gamma distributions
dt_u <- data.table(lookup_u)
dt_r <- data.table(lookup_r)
dt_s <- data.table(lookup_s)

setkey(dt_u, lookup_u)
setkey(dt_r, lookup_r)
setkey(dt_s, lookup_s)

params <- setDT(crossing(Na, N1, t, lookup_r, lookup_s, lookup_u))
params$scenario <- 1:nrow(params)
n_models <- nrow(params)

fwrite(dt_u, "u.csv", col.names=F)
fwrite(dt_r, "r.csv", col.names=F)
fwrite(dt_s, "s.csv", col.names=F)
fwrite(params, "params.csv")
```

```{bash dirs, engine.opts='-i', include=F, eval=T, results=F, warning=F}

# NOTE: this chunk requires that the directories do not already exist
source ~/.bashrc
  
cat r.csv | while IFS=, read -r r
do
  mkdir r_$r
  cat s.csv | while IFS=, read -r s
  do
    mkdir r_$r/s_$s
    cat u.csv | while IFS=, read -r u
    do
      mkdir r_$r/s_$s/u_$u
    done
  done
done

find -mindepth 3 -type d -exec cp opt.bpp {} \;

# makes directory to store purely neutral models to get pi0 predictions
mkdir demo
cp opt.bpp demo
```

Creates files that will be used as input for moments++

```{r, demes, include=F, eval=T, results=F, message=F, warning=F}

# writes demes files specifying models
for(i in 1:n_models) {
  
  if(i %% 10000 == 0) {
    print(Sys.time())
    cat(paste(i, "\n"))
  }
  
  Na <- as.integer(params[i,]$Na)
  N1 <- as.integer(params[i,]$N1)
  t <- as.integer(params[i,]$t)
  u <- as.numeric(params[i,]$lookup_u)
  r <- as.numeric(params[i,]$lookup_r)
  s <- as.numeric(params[i,]$lookup_s)
  
  name <- paste("model_", i, sep="")
  sink(paste("r_", r, 
             "/s_", s,
             "/u_", u,
             "/", name, ".yaml", sep=""))
  
  cat("time_units: generations\n")
  cat("demes:\n")
  cat("  - name: X\n")
  cat("    epochs:\n")
  if(N1 != Na) {
    cat("      - end_time: ")
    cat(t)
    cat("\n")
    cat("        start_size: ")
    cat(Na)
    cat("\n")
  }
  cat("      - end_time: 0\n")
  cat("        start_size: ")
  cat(N1)
  cat("\n")
  cat("metadata:\n")
  cat("  - name: mutation\n")
  cat("    left_factor: 1\n")
  cat("    epochs:\n")
  cat("      - end_time: 0\n")
  cat("        rates: [")
  cat(u)
  cat("]\n")
  cat("  - name: recombination\n")
  cat("    epochs:\n")
  cat("      - end_time: 0\n")
  cat("        rates: [")
  cat(r)
  cat("]\n")
  cat("  - name: selection\n")
  cat("    start_time: .inf\n")
  cat("    epochs:\n")
  cat("      - end_time: 0\n")
  cat("        rates: [")
  cat(format(s, scientific=F))
  cat("]\n")
  
  sink()
}

# creates (purely neutral) demographic models
Na <- as.integer(unique(params$Na))
N1 <- as.integer(unique(params$N1))
t <- as.integer(unique(params$t))
u <- as.numeric(unique(params$lookup_u))

c <- 1
for(N in N1) {
  for(mu in u) {
    sink(paste("demo/model_", c, ".yaml", sep=""))
    cat("time_units: generations\n")
    cat("demes:\n")
    cat("  - name: X\n")
    cat("    epochs:\n")
    if(N1 != Na) {
      cat("      - end_time: ")
      cat(t)
      cat("\n")
      cat("        start_size: ")
      cat(Na)
      cat("\n")
    }
    cat("      - end_time: 0\n")
    cat("        start_size: ")
    cat(N)
    cat("\n")
    cat("metadata:\n")
    cat("  - name: mutation\n")
    cat("    left_factor: 1\n")
    cat("    epochs:\n")
    cat("      - end_time: 0\n")
    cat("        rates: [")
    cat(mu)
    cat("]\n")
    cat("  - name: recombination\n")
    cat("    epochs:\n")
    cat("      - end_time: 0\n")
    cat("        rates: [0]\n")
    cat("  - name: selection\n")
    cat("    start_time: .inf\n")
    cat("    epochs:\n")
    cat("      - end_time: 0\n")
    cat("        rates: [0]\n")
    sink()
    
    c <- c + 1
  }
}
  
```

# Running moments++

Computes summary statistics for each parameter combination in lookup_tbl

```{bash moments++, engine.opts='-i', include=F, eval=F, results=F, warning=F}

#!/bin/bash

for r in r_*
do
  date
  cd $r
  for s in s_*
  do
    cd $s
    for u in u_*
    do
      cd $u
      for f in model_*.yaml
      do
	      N=$(sed '8q;d' $f | cut -d : -f 2 | cut -d ' ' -f 2)
        x=$(sed '23q;d' $f | cut -d : -f 2 | cut -d [ -f 2 | cut -d ] -f 1)
        s=$(echo "scale=10; $x" | bc)
	      a=$(echo "scale=10; $N * $s" | bc)
	      if [[ $(echo "$a < -1" | bc) == 1 ]]
	      then
	        b=${a%.*}
	        z=$(($b * -1))
	        w=$((8 * $z))
  	      momentspp params=opt.bpp F=$f O=$w > /dev/null
        else
	        momentspp params=opt.bpp F=$f O=10 > /dev/null
	      fi
      done
      cd ..
    done
    cd ..
  done
  cd ..
done


cd demo
for f in model_*
do 
  momentspp params=opt.bpp F=$f O=2
done
```

Collects lookup table with Hr values on the parameter grid

```{r, grid, include=F, eval=F, results=F, message=F, warning=F}

params <- fread("params.csv")

lookup_r <- unique(params$lookup_r)
lookup_s <- unique(params$lookup_s)
lookup_u <- unique(params$lookup_u)

n_models <- nrow(params)
t <- unique(params$t)
sampling_times <- seq(from=t, to=0, by=-250)

# Het stats over time
hrs <- as.data.frame(matrix(nrow=n_models, ncol=length(sampling_times)))
hls <- as.data.frame(matrix(nrow=n_models, ncol=length(sampling_times)))

names(hrs) <- as.character(sampling_times)
names(hls) <- as.character(sampling_times)

# Het stats at present time
hr <- numeric(length=n_models)
hl <- numeric(length=n_models)

for(i in 1:length(lookup_r)) {
  cat(paste(i, "\n"))
  print(Sys.time())
  for(j in 1:length(lookup_s)) {
    for(k in 1:length(lookup_u)) {
      fnames <- list.files(paste("r_", lookup_r[i], "/s_", lookup_s[j], 
                                 "/u_", lookup_u[k], sep=""),
                           pattern="*e_1_expectations.txt", full.names=TRUE)
      
      for(l in 1:length(fnames)) {
        moms <- fread(fnames[l])
        idx <- as.numeric(strsplit(strsplit(fnames[l], "_e_1")[[1]][1], 
                                   "model_")[[1]][2])
        
        hls[idx,] <- t(dplyr::select(filter(moms, V1=="Hl_0_0"), V3))
        hrs[idx,] <- t(dplyr::select(filter(moms, V1=="Hr_0_0"), V3))
        
        # expectations at present time
        hl[idx] <- moms[nrow(moms) - 1, 3]
        hr[idx] <- moms[nrow(moms), 3]
      }
    }
  }
}

params_hl <- cbind.data.frame(params, hls)
params_hr <- cbind.data.frame(params, hrs)

fwrite(params_hl, "hl_time.csv.gz", compress="gzip") # used by other scripts
fwrite(params_hr, "hr_time.csv.gz", compress="gzip") # used by other scripts

py$df <- params_hr # stores df to be used in python sections
```

```{python run-momentsLD, include=F, eval=F, results=F}

import pandas
from scipy import interpolate
import numpy as np
import matplotlib.pylab as plt

# just the expansion scenario
df = df[df["N1"] == 100000]

# parameters
u = 1e-8
r = 1e-8
L = 100000
Ne = 10000

gen = 50000

pi0 = 2 * Ne * u

s = -0.001

# set up windows and "exons" -- assertions are just to evenly grid the sequence length
spacing = 1000
assert L % spacing == 0

exon_L = 100
assert L % exon_L == 0

exon_mids = np.arange(exon_L / 2, L - exon_L / 2 + 1, exon_L)
assert len(exon_mids) == L // exon_L

xx = np.arange(0, L + 1, spacing)

# build cubic splines
s_vals = np.array(sorted(list(set(df["lookup_s"]))))
s_splines = {}
for s_val in s_vals:
    df_s = df[df["lookup_s"] == s_val]
    rs = np.array(df_s["lookup_r"])
    Hs = np.array(df_s[str(gen)])
    s_splines[s_val] = interpolate.CubicSpline(rs, Hs, bc_type="natural")


#### compute B values

# initial B value
B_vals = np.zeros(len(xx))
for i, x in enumerate(xx):
    B_vals[i] = np.prod((s_splines[s](np.abs(x - exon_mids) * r) / pi0) ** exon_L)


def B_interference_iteration(xx, B_func, u, r, s):
    """
    Note that this is really rough and could be made better in a lot of ways.....

    xx: positions to compute B value for
    B_func: cubic spline function interpolating B values from those computed at xx
    u: initial mutation rate
    r: initial recombination rate
    s: selection coefficient
    """
    B_mids = B_func(exon_mids)
    # adjusted cumulative r
    r_exon = B_mids * r
    r_cum = [0]
    for r_e in r_exon:
        r_cum.append(r_cum[-1] + r_e * exon_L)
    r_cum = np.array(r_cum)

    r_mids = (r_cum[1:] + r_cum[:-1]) / 2
    r_xx = np.concatenate(
        ([0], r_cum[1:].reshape(L // spacing, spacing // exon_L)[:, -1])
    )

    # adjusted mut within each exon
    u_exon = B_mids * u
    # adjusted s within each exon
    s_exon = B_mids * s
    ## note in future would pick weights between two closest?
    # s_closest = [s_vals[np.argmin(np.abs(s_vals - s_e))] for s_e in s_exon]
    s_low = np.array([s_vals[np.where(s_e >= s_vals)[0][-1]] for s_e in s_exon])
    s_high = np.array([s_vals[np.where(s_e < s_vals)[0][0]] for s_e in s_exon])
    p_high = (s_exon - s_low) / (s_high - s_low)
    p_low = 1 - p_high

    B_vals_interference = np.zeros(len(xx))
    for i, x in enumerate(xx):
        B_i = 1
        r_x = r_xx[i]
        r_dists = np.abs(r_mids - r_x)
        for j, (r_e, u_e) in enumerate(zip(r_dists, u_exon)):
            # B_i *= (s_splines[s_e](r_e) / pi0) ** (exon_L * u_e / u)
            p_l = p_low[j]
            p_h = p_high[j]
            s_l = s_low[j]
            s_h = s_high[j]
            B_l = (s_splines[s_l](r_e) / pi0) ** (exon_L * u_e / u)
            B_h = (s_splines[s_h](r_e) / pi0) ** (exon_L * u_e / u)
            B_i *= p_l * B_l + p_h * B_h
        B_vals_interference[i] = B_i
    return B_vals_interference


# run for some iterations
B_iter = {0: B_vals}
for i in range(1, 11):
    print("running iteration", i)
    B_func = interpolate.CubicSpline(xx, B_iter[i - 1], bc_type="natural")
    B_iter[i] = B_interference_iteration(xx, B_func, u, r, s)
```
