---
title: "BGS under non-equilibrium demography (paper 1)"
author: "Gustavo V. Barroso"
date: "`r Sys.Date()`"
output:
  pdf_document:
  toc: true
number_sections: true
toc_depth: 1
---
  
```{r setup, include=F, message=F, warning=F}

knitr::opts_chunk$set(echo=TRUE)

library(tidyverse)
library(bigsnpr) # for seq_log
library(data.table)

knitr::opts_knit$set(root.dir="~/Data/momentspp/paper_1/study")
```

This is an empty python chunk.
Apparently this must exist in order for reticulate to be able to read
R data frames in future python chunks.

```{python}
```

# Creating folders and files

This builds a look-up table on a dense parameter grid that will allow for fast 
assembly of whole chromosomes

```{r, params, include=F, eval=T, results=F, message=F, warning=F}

Na <- 1e+4 # ancestral N
N1 <- c(Na / 10, 10 * Na) # current N
N2 <- c(N1, Na)
t <- 100000 # in generations
u <- c(1e-8) # constant among exons as well as "fake" neutral u used to get B-vals

# rounding is required so that directory names don't diverge due to precision
lookup_r <- round(sort(c(seq_log(from=1e-8, to=1e-3, length.out=20),
                  seq(from=1e-4, to=1e-2, length.out=20))), digits=10)
lookup_s <- round(-sort(c(seq_log(from=1e-5, to=1e-4, length.out=40), 
                    seq_log(from=7.5e-5, to=1e-3, length.out=80))), digits=10)

# for discretizing Gamma distributions
dt_r <- data.table(lookup_r)
dt_s <- data.table(lookup_s)

setkey(dt_r, lookup_r)
setkey(dt_s, lookup_s)

params <- setDT(crossing(Na, N1, t, u, lookup_r, lookup_s))
params$scenario <- 1:nrow(params)
n_models <- nrow(params)

fwrite(dt_r, "r.csv", quote=F, row.names=F, col.names=F)
fwrite(dt_s, "s.csv", quote=F, row.names=F, col.names=F)
fwrite(params, "params.csv", quote=F, row.names=F)
```

```{bash dirs, engine.opts='-i', include=F, eval=T, results=F, warning=F}

# NOTE: this chunk requires that the directories do not already exist
source ~/.bashrc
  
cat r.csv | while IFS=, read -r r
do
  mkdir r_$r
  
  cat s.csv | while IFS=, read -r s
  do
    mkdir r_$r/s_$s
  done
done

find -mindepth 2 -type d -exec cp opt.bpp {} \;
```

Creates files that will be used as input for moments++

```{r, demes, include=F, eval=T, results=F, message=F, warning=F}

# writes demes files specifying models
for(i in 1:n_models) {
  
  if(i %% 10000 == 0) {
    print(Sys.time())
    cat(paste(i, "\n"))
  }
  
  Na <- as.integer(params[i,]$Na)
  N1 <- as.integer(params[i,]$N1)
  t <- as.integer(params[i,]$t)
  u <- as.numeric(params[i,]$u)
  r <- as.numeric(params[i,]$lookup_r)
  s <- as.numeric(params[i,]$lookup_s)
  
  name <- paste("model_", i, sep="")
  sink(paste("r_", r, "/s_", s, "/", name, ".yaml", sep=""))
  
  cat("time_units: generations\n")
  cat("demes:\n")
  cat("  - name: X\n")
  cat("    epochs:\n")
  cat("      - end_time: ")
  cat(t)
  cat("\n")
  cat("        start_size: ")
  cat(Na)
  cat("\n")
  cat("      - end_time: 0\n")
  cat("        start_size: ")
  cat(N1)
  cat("\n")
  cat("metadata:\n")
  cat("  - name: mutation\n")
  cat("    left_factor: 1\n")
  cat("    epochs:\n")
  cat("      - end_time: 0\n")
  cat("        rates: [")
  cat(u)
  cat("]\n")
  cat("  - name: recombination\n")
  cat("    epochs:\n")
  cat("      - end_time: 0\n")
  cat("        rates: [")
  cat(r)
  cat("]\n")
  cat("  - name: selection\n")
  cat("    start_time: .inf\n")
  cat("    epochs:\n")
  cat("      - end_time: 0\n")
  cat("        rates: [")
  cat(s)
  cat("]\n")
  
  sink()
}
```

# Running moments++

Computes summary statistics for each parameter combination in lookup_tbl

```{bash moments++, engine.opts='-i', include=F, eval=F, results=F, warning=F}

#!/bin/bash

for r in r_*
do
  cd $r
  for s in s_*
  do
    cd $s
    for f in model_*.yaml
    do
      # this is to get the correct Order of (1-2p)
      # which is a function of 2Ns (maximum = 2N/1000 here)
      N=$(sed '8q;d' $f | cut -d : -f 2 | cut -d ' ' -f 2)
      x=$((2*$N/1000))
      if [[ $x -gt 90 ]]
      then
        momentspp params=opt.bpp F=$f O=$x > /dev/null
      else
        momentspp params=opt.bpp F=$f O=50
      fi
    done
    cd ..
  done
  cd ..
done
```

Collects lookup table with Hr values on the parameter grid

```{r, grid, include=F, eval=F, results=F, message=F, warning=F}

params <- fread("params.csv")

lookup_r <- unique(params$lookup_r)
lookup_s <- unique(params$lookup_s)

n_models <- nrow(params)
t <- unique(params$t)
sampling_times <- seq(from=t, to=0, by=-1000)

# Het stats over time
hrs <- as.data.frame(matrix(nrow=n_models, ncol=length(sampling_times)))
hls <- as.data.frame(matrix(nrow=n_models, ncol=length(sampling_times)))

names(hrs) <- as.character(sampling_times)
names(hls) <- as.character(sampling_times)

# Het stats at present time
hr <- numeric(length=n_models)
hl <- numeric(length=n_models)

for(i in 1:length(lookup_r)) {
  cat(paste(i, "\n"))
  print(Sys.time())
  for(j in 1:length(lookup_s)) {
    fnames <- list.files(paste("r_", lookup_r[i], "/s_", lookup_s[j], sep=""),
                         pattern="*e_1_expectations.txt", full.names=TRUE)
    
    for(k in 1:length(fnames)) {
      moms <- fread(fnames[k])
      idx <- as.numeric(strsplit(strsplit(fnames[k], "_e_1")[[1]][1], 
                                 "model_")[[1]][2])
      
      hls[idx,] <- t(dplyr::select(filter(moms, V1=="Hl_0_0"), V3))
      hrs[idx,] <- t(dplyr::select(filter(moms, V1=="Hr_0_0"), V3))
      
      # expectations at present time
      hl[idx] <- moms[nrow(moms) - 1, 3]
      hr[idx] <- moms[nrow(moms), 3]
    }
  }
}

params_hl <- cbind.data.frame(params, hls)
params_hr <- cbind.data.frame(params, hrs)

fwrite(params_hl, "hl_time.csv.gz", compress="gzip") # used by other scripts
fwrite(params_hr, "hr_time.csv.gz", compress="gzip") # used by other scripts

py$df <- params_hr # stores df to be used in python sections
```

```{python run-momentsLD, include=F, eval=F, results=F}

import pandas
from scipy import interpolate
import numpy as np
import matplotlib.pylab as plt

# just the expansion scenario
df = df[df["N1"] == 100000]

# parameters
u = 1e-8
r = 1e-8
L = 100000
Ne = 10000

gen = 50000

pi0 = 2 * Ne * u

s = -0.001

# set up windows and "exons" -- assertions are just to evenly grid the sequence length
spacing = 1000
assert L % spacing == 0

exon_L = 100
assert L % exon_L == 0

exon_mids = np.arange(exon_L / 2, L - exon_L / 2 + 1, exon_L)
assert len(exon_mids) == L // exon_L

xx = np.arange(0, L + 1, spacing)

# build cubic splines
s_vals = np.array(sorted(list(set(df["lookup_s"]))))
s_splines = {}
for s_val in s_vals:
    df_s = df[df["lookup_s"] == s_val]
    rs = np.array(df_s["lookup_r"])
    Hs = np.array(df_s[str(gen)])
    s_splines[s_val] = interpolate.CubicSpline(rs, Hs, bc_type="natural")


#### compute B values

# initial B value
B_vals = np.zeros(len(xx))
for i, x in enumerate(xx):
    B_vals[i] = np.prod((s_splines[s](np.abs(x - exon_mids) * r) / pi0) ** exon_L)


def B_interference_iteration(xx, B_func, u, r, s):
    """
    Note that this is really rough and could be made better in a lot of ways.....

    xx: positions to compute B value for
    B_func: cubic spline function interpolating B values from those computed at xx
    u: initial mutation rate
    r: initial recombination rate
    s: selection coefficient
    """
    B_mids = B_func(exon_mids)
    # adjusted cumulative r
    r_exon = B_mids * r
    r_cum = [0]
    for r_e in r_exon:
        r_cum.append(r_cum[-1] + r_e * exon_L)
    r_cum = np.array(r_cum)

    r_mids = (r_cum[1:] + r_cum[:-1]) / 2
    r_xx = np.concatenate(
        ([0], r_cum[1:].reshape(L // spacing, spacing // exon_L)[:, -1])
    )

    # adjusted mut within each exon
    u_exon = B_mids * u
    # adjusted s within each exon
    s_exon = B_mids * s
    ## note in future would pick weights between two closest?
    # s_closest = [s_vals[np.argmin(np.abs(s_vals - s_e))] for s_e in s_exon]
    s_low = np.array([s_vals[np.where(s_e >= s_vals)[0][-1]] for s_e in s_exon])
    s_high = np.array([s_vals[np.where(s_e < s_vals)[0][0]] for s_e in s_exon])
    p_high = (s_exon - s_low) / (s_high - s_low)
    p_low = 1 - p_high

    B_vals_interference = np.zeros(len(xx))
    for i, x in enumerate(xx):
        B_i = 1
        r_x = r_xx[i]
        r_dists = np.abs(r_mids - r_x)
        for j, (r_e, u_e) in enumerate(zip(r_dists, u_exon)):
            # B_i *= (s_splines[s_e](r_e) / pi0) ** (exon_L * u_e / u)
            p_l = p_low[j]
            p_h = p_high[j]
            s_l = s_low[j]
            s_h = s_high[j]
            B_l = (s_splines[s_l](r_e) / pi0) ** (exon_L * u_e / u)
            B_h = (s_splines[s_h](r_e) / pi0) ** (exon_L * u_e / u)
            B_i *= p_l * B_l + p_h * B_h
        B_vals_interference[i] = B_i
    return B_vals_interference


# run for some iterations
B_iter = {0: B_vals}
for i in range(1, 11):
    print("running iteration", i)
    B_func = interpolate.CubicSpline(xx, B_iter[i - 1], bc_type="natural")
    B_iter[i] = B_interference_iteration(xx, B_func, u, r, s)
```
